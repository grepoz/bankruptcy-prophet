{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "## Part 3 - looking for tickers, exchange and stockid using gurufocus.com to downlod data in the next step\n",
    "I need to get tickers, exchange and stockid for companies - to further download financial data from gurufocus.com (or look for different data provider). Also I focus on NYSE and NASDAQ exchanges."
   ],
   "id": "844559cc42f4cbde"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# ",
   "id": "fe7899dd5246776f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-19T19:44:30.262015Z",
     "start_time": "2024-04-19T19:44:29.878265Z"
    }
   },
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
    "    'authority': 'www.google.com',\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'max-age=0',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36',\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T19:44:33.092415Z",
     "start_time": "2024-04-19T19:44:33.087166Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pattern = re.compile(r\"[\\\\/.,\\\"]\") \n",
    "\n",
    "def clear_company_name(company_name):\n",
    "    company_name = company_name.lower()\n",
    "    company_name = re.sub(r\"[\\\\/(].*\", '', company_name)\n",
    "    company_name = pattern.sub('', company_name)\n",
    "    company_name = company_name.strip()\n",
    "    company_name = company_name.replace(' ', '+')\n",
    "    return company_name"
   ],
   "id": "c28bae9b308bce11",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_datetime_now():\n",
    "    return datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ],
   "id": "c76cb09bba7dacce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "dataset = pd.read_csv('ECL.csv', index_col=0)",
   "id": "4175b8d346dd1204"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "datetime_of_initial_processing = datetime.datetime.strptime('2021-10-01 00:00:00', \"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "prediction_subset = pd.read_csv(f'ECL_{datetime_of_initial_processing}.csv', index_col=0)\n",
    "unique_companies = pd.read_csv(f'ECL_unique_companies_{datetime_of_initial_processing}.csv', index_col=0)"
   ],
   "id": "8e1c0201664d7fa2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "found_companies_count = 0\n",
    "\n",
    "companies_with_not_found_tickers_list = []\n",
    "companies_with_not_found_tickers_with_exception = []\n",
    "\n",
    "pbar = tqdm(unique_companies.values.tolist())\n",
    "\n",
    "exceptions_list = []\n",
    "\n",
    "for cik, company in pbar:\n",
    "\n",
    "    cleared_company_name = clear_company_name(company)\n",
    "    url = f'https://www.gurufocus.com/reader/_api/_search?v=1.4.13&text={cleared_company_name}'\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            response_json = response.json()\n",
    "        except Exception as ex:\n",
    "            companies_with_not_found_tickers_with_exception.append({'cik': cik, 'company': company})\n",
    "            exceptions_list.append(ex)\n",
    "            time.sleep(0.7)\n",
    "            continue\n",
    "\n",
    "        time.sleep(0.2)\n",
    "    \n",
    "        if len(response_json) == 0:\n",
    "            has_company_name_changed = False\n",
    "            if '+inc' in cleared_company_name:\n",
    "                cleared_company_name = cleared_company_name.replace('+inc', '')\n",
    "                has_company_name_changed = True\n",
    "            if '+co' in cleared_company_name:\n",
    "                # may match 'co' in any word\n",
    "                cleared_company_name = cleared_company_name.replace('+co', '+company')\n",
    "                has_company_name_changed = True\n",
    "            if '+llc' in cleared_company_name:\n",
    "                cleared_company_name = cleared_company_name.replace('+llc', '')\n",
    "                has_company_name_changed = True\n",
    "            cleared_company_name = cleared_company_name.strip()\n",
    "            \n",
    "            if has_company_name_changed:\n",
    "                url = f'https://www.gurufocus.com/reader/_api/_search?v=1.4.13&text={cleared_company_name}'\n",
    "                try:\n",
    "                    response = requests.get(url, headers=headers)\n",
    "                    response_json = response.json()\n",
    "                except Exception as ex:\n",
    "                    companies_with_not_found_tickers_with_exception.append({'cik': cik, 'company': company})\n",
    "                    exceptions_list.append(ex)\n",
    "                    time.sleep(0.7)\n",
    "                    continue\n",
    "                \n",
    "            if len(response_json) == 0:\n",
    "                companies_with_not_found_tickers_list.append({'cik': cik, 'company': company})\n",
    "                continue\n",
    "        \n",
    "        is_company_found = False\n",
    "        matched_type_counter = 0\n",
    "        for entry in response_json:\n",
    "            if entry['type'] not in ('stock', 'delisted'):\n",
    "                if matched_type_counter > 0:\n",
    "                    break\n",
    "                continue\n",
    "                \n",
    "            exchange = entry['data']['exchange']\n",
    "            if exchange in ('NYSE', 'NAS', 'DELISTED'):\n",
    "                ticker = entry['data']['symbol']\n",
    "                stockid = entry['data']['stockid']\n",
    "                # gurufocus_company_name = entry['data']['company']\n",
    "                \n",
    "                if matched_type_counter == 0:\n",
    "                    prediction_subset.loc[prediction_subset['cik'] == cik, 'ticker'] = ticker\n",
    "                    prediction_subset.loc[prediction_subset['cik'] == cik, 'exchange'] = exchange\n",
    "                    prediction_subset.loc[prediction_subset['cik'] == cik, 'gurufocus-stockid'] = stockid\n",
    "                    # prediction_subset.loc[prediction_subset['cik'] == cik, 'gurufocus-company-name'] = gurufocus_company_name\n",
    "                    \n",
    "                    \n",
    "                    is_company_found = True\n",
    "                    found_companies_count += 1\n",
    "                    pbar.set_description(f\"Found companies count: {found_companies_count}\")\n",
    "                    matched_type_counter += 1\n",
    "                    \n",
    "                elif matched_type_counter == 1:\n",
    "                    prediction_subset.loc[prediction_subset['cik'] == cik, 'second-match-ticker'] = ticker\n",
    "                    prediction_subset.loc[prediction_subset['cik'] == cik, 'second-match-exchange'] = exchange\n",
    "                    prediction_subset.loc[prediction_subset['cik'] == cik, 'second-match-gurufocus-stockid'] = stockid\n",
    "                    # prediction_subset.loc[prediction_subset['cik'] == cik, 'second-match-gurufocus-company-name'] = gurufocus_company_name\n",
    "                else:\n",
    "                    break\n",
    "                \n",
    "        if not is_company_found:\n",
    "            companies_with_not_found_tickers_list.append({'cik': cik, 'company': company})\n",
    "            \n",
    "    except Exception as ex:\n",
    "        companies_with_not_found_tickers_with_exception.append({'cik': cik, 'company': company})\n",
    "        exceptions_list.append(ex)\n",
    "        time.sleep(0.7)\n",
    "        \n",
    "companies_with_not_found_tickers_df = pd.DataFrame(companies_with_not_found_tickers_list)\n",
    "companies_with_not_found_tickers_with_exception_df = pd.DataFrame(companies_with_not_found_tickers_with_exception)"
   ],
   "id": "5a839f6159910287"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Number of companies with found tickers: {found_companies_count}\")\n",
    "print(f\"Number of companies with not found tickers: {len(companies_with_not_found_tickers_list)}\")\n",
    "print(f\"Number of companies with exceptions: {len(companies_with_not_found_tickers_with_exception_df)}\")"
   ],
   "id": "b8e3eb4389577944"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "datetime_now = get_datetime_now()\n",
    "\n",
    "prediction_subset.to_csv(f'ECL_with_ticker_{datetime_now}.csv')\n",
    "companies_with_not_found_tickers_df.to_csv(f'ECL_companies_with_not_found_tickers_{datetime_now}.csv')\n",
    "companies_with_not_found_tickers_with_exception_df.to_csv(f'ECL_companies_with_not_found_tickers_with_exception_{datetime_now}.csv')"
   ],
   "id": "e0a2f70582a19ec9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Download data from gurufocus.com\n",
    "#### Download data for companies with one ticker (then for companies with two tickers)"
   ],
   "id": "2f8d51dceff77da0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "prediction_subset = pd.read_csv('ECL_with_ticker_2024-04-14_15-20-33.csv', index_col=0)",
   "id": "64f1992aac8aef12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prediction_subset_with_gurufocus_data = prediction_subset.groupby('cik').agg(\n",
    "    company=('company', 'last'),\n",
    "    ticker=('ticker', 'last'),\n",
    "    second_match_ticker=('second-match-ticker', 'last'),\n",
    "    gurufocus_stockid=('gurufocus-stockid', 'last'),\n",
    "    second_match_gurufocus_stockid=('second-match-gurufocus-stockid', 'last')\n",
    ").reset_index()"
   ],
   "id": "bf9c05a2a65f153"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# get companies with one ticker\n",
    "companies_with_one_ticker = prediction_subset_with_gurufocus_data[prediction_subset_with_gurufocus_data['ticker'].notnull() & prediction_subset_with_gurufocus_data['second_match_ticker'].isnull()]\n",
    "print(len(companies_with_one_ticker))\n",
    "print(companies_with_one_ticker.head(10))"
   ],
   "id": "4b4716a7c74b3455"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "companies_with_one_ticker_grouped_by_cik = companies_with_one_ticker.groupby('cik').agg(\n",
    "    ticker=('ticker', 'last'),\n",
    "    gurufocus_stockid=('gurufocus_stockid', 'last')\n",
    ").reset_index()\n",
    "print(len(companies_with_one_ticker_grouped_by_cik))\n",
    "print(companies_with_one_ticker_grouped_by_cik.head(10))"
   ],
   "id": "5119c1a29126d382"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T19:44:40.746756Z",
     "start_time": "2024-04-19T19:44:40.599824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "table_ids = [\n",
    "    'financial_table_per_share_data',\n",
    "    'financial_table_ratios',\n",
    "    'financial_table_income_statement',\n",
    "    'financial_table_balance_sheet',\n",
    "    'financial_table_cashflow_statement',\n",
    "    'financial_table_valuation_ratios',\n",
    "    'financial_table_valuation_and_quality'\n",
    "]\n",
    "\n",
    "options = webdriver.EdgeOptions()\n",
    "\n",
    "login_url = 'https://www.gurufocus.com/login'\n",
    "payload = {\n",
    "    \"username\": \"darekkruszel15@gmail.com\",\n",
    "    \"password\": \"=OcUZ*5&|{+l7-lGy:QMe4vHyF4X'~\"\n",
    "}\n",
    "\n",
    "def process_df(df):\n",
    "    df.drop(df.columns[1], axis=1, inplace=True) \n",
    "    return df.loc[:, ~df.columns.str.contains('^Unnamed')]"
   ],
   "id": "a1e7a079840345ee",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# login\n",
    "driver = webdriver.Edge(options=options)\n",
    "driver.get(login_url)\n",
    "\n",
    "username = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, 'username')))\n",
    "password = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, 'password')))\n",
    "username.clear()\n",
    "password.clear()\n",
    "username.send_keys(payload['username'])\n",
    "password.send_keys(payload['password'])\n",
    "password.send_keys(Keys.RETURN)\n",
    "\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.LINK_TEXT, 'Articles')))\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "not_found_financial_data_with_exception = []\n",
    "not_found_financial_data_with_webdriver_timeout_exception = []\n",
    "success_count = 0\n",
    "\n",
    "pbar = tqdm(companies_with_one_ticker_grouped_by_cik.values.tolist())\n",
    "try:\n",
    "    for cik, ticker, stockid in pbar:\n",
    "        \n",
    "        url = f'https://www.gurufocus.com/stock/{stockid}/financials'\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(0.7)\n",
    "    \n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_per_share_data')))\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_ratios')))\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_income_statement')))\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_balance_sheet')))\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_cashflow_statement')))\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_valuation_ratios')))\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_valuation_and_quality')))\n",
    "            except Exception as ex:\n",
    "                not_found_financial_data_with_webdriver_timeout_exception.append([cik, stockid, ticker, str(ex)])\n",
    "                continue\n",
    "    \n",
    "            page_source = driver.page_source\n",
    "            with open(f'gurufocus-immediate-response-for-stockid/page_source-{cik}-{stockid}_{ticker}.txt', 'w', encoding='utf-8') as f:\n",
    "                f.write(page_source)\n",
    "            \n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "            gurufocus_company_name = soup.find('div', {'id': 'stock-header'}).find('div').text\n",
    "    \n",
    "            prediction_subset.loc[prediction_subset['cik'] == cik, 'gurufocus-company-name'] = gurufocus_company_name\n",
    "        \n",
    "            tables = soup.find_all('table')\n",
    "            \n",
    "            page_tables_ids = []\n",
    "            for table in tables:\n",
    "                try:\n",
    "                    table_id = table.get('id')\n",
    "                    if table_id:\n",
    "                        page_tables_ids.append(table_id)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if all(table_id not in page_tables_ids for table_id in table_ids):\n",
    "                pass\n",
    "            else:\n",
    "                merged_df = pd.DataFrame()\n",
    "                for table_id in table_ids:\n",
    "                    table = soup.find(id=table_id)\n",
    "                    df = pd.read_html(str(table), skiprows=1, header=0)[0]\n",
    "                    df = process_df(df)\n",
    "                    merged_df = pd.concat([merged_df, df])\n",
    "                    \n",
    "                merged_df.reset_index(drop=True, inplace=True)\n",
    "                merged_df.to_csv(f'financial_data/{cik}-{stockid}_{ticker}.csv')\n",
    "                \n",
    "                success_count += 1\n",
    "                pbar.set_description(f\"Success: {success_count}\")\n",
    "    \n",
    "        except Exception as ex:\n",
    "            not_found_financial_data_with_exception.append([cik, stockid, ticker, str(ex)])\n",
    "\n",
    "finally:\n",
    "    driver.close()\n",
    "    prediction_subset.to_csv(f'ECL_with_ticker_{datetime_now}.csv')\n",
    "\n",
    "    with open('not_found_financial_data_with_exception.json', 'w') as file:\n",
    "        json.dump(list(not_found_financial_data_with_exception), file)\n",
    "        \n",
    "    with open('not_found_financial_data_with_webdriver_timeout_exception.json', 'w') as file:\n",
    "        json.dump(list(not_found_financial_data_with_webdriver_timeout_exception), file)"
   ],
   "id": "45c2f19d58c2380e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T19:48:26.025405Z",
     "start_time": "2024-04-19T19:48:12.510992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# login\n",
    "driver = webdriver.Edge(options=options)\n",
    "driver.get(login_url)\n",
    "\n",
    "username = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, 'username')))\n",
    "password = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, 'password')))\n",
    "username.clear()\n",
    "password.clear()\n",
    "username.send_keys(payload['username'])\n",
    "password.send_keys(payload['password'])\n",
    "password.send_keys(Keys.RETURN)\n",
    "\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.LINK_TEXT, 'Articles')))\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "not_found_financial_data_with_exception = []\n",
    "not_found_financial_data_with_webdriver_timeout_exception = []\n",
    "success_count = 0\n",
    "\n",
    "try:\n",
    "    stockid = 'US08YY'\n",
    "    ticker = 'PCG'\n",
    "    cik = 75488\n",
    "    # \"exchange\": \"NYSE\",\n",
    "        \n",
    "    url = f'https://www.gurufocus.com/stock/{stockid}/financials'\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(0.7)\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_per_share_data')))\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_ratios')))\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_income_statement')))\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_balance_sheet')))\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_cashflow_statement')))\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_valuation_ratios')))\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_valuation_and_quality')))\n",
    "        except Exception as ex:\n",
    "            not_found_financial_data_with_webdriver_timeout_exception.append([cik, stockid, ticker, str(ex)])\n",
    "\n",
    "        page_source = driver.page_source\n",
    "        with open(f'gurufocus-immediate-response-for-stockid/page_source-{cik}-{stockid}_{ticker}.txt', 'w', encoding='utf-8') as f:\n",
    "            f.write(page_source)\n",
    "        \n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    \n",
    "        tables = soup.find_all('table')\n",
    "        \n",
    "        page_tables_ids = []\n",
    "        for table in tables:\n",
    "            try:\n",
    "                table_id = table.get('id')\n",
    "                if table_id:\n",
    "                    page_tables_ids.append(table_id)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        if all(table_id not in page_tables_ids for table_id in table_ids):\n",
    "            pass\n",
    "        else:\n",
    "            merged_df = pd.DataFrame()\n",
    "            for table_id in table_ids:\n",
    "                table = soup.find(id=table_id)\n",
    "                df = pd.read_html(str(table), skiprows=1, header=0)[0]\n",
    "                df = process_df(df)\n",
    "                merged_df = pd.concat([merged_df, df])\n",
    "                \n",
    "            merged_df.reset_index(drop=True, inplace=True)\n",
    "            merged_df.to_csv(f'final_financial_data/{cik}-{stockid}_{ticker}.csv')\n",
    "            \n",
    "            success_count += 1\n",
    "            pbar.set_description(f\"Success: {success_count}\")\n",
    "\n",
    "    except Exception as ex:\n",
    "        not_found_financial_data_with_exception.append([cik, stockid, ticker, str(ex)])\n",
    "\n",
    "finally:\n",
    "    driver.close()\n",
    "\n",
    "    with open('not_found_financial_data_with_exception.json', 'w') as file:\n",
    "        json.dump(list(not_found_financial_data_with_exception), file)\n",
    "        \n",
    "    with open('not_found_financial_data_with_webdriver_timeout_exception.json', 'w') as file:\n",
    "        json.dump(list(not_found_financial_data_with_webdriver_timeout_exception), file)"
   ],
   "id": "c2b12feb5b8fadc4",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction_subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 82\u001B[0m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     81\u001B[0m     driver\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m---> 82\u001B[0m     \u001B[43mprediction_subset\u001B[49m\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mECL_with_ticker_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdatetime_now\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnot_found_financial_data_with_exception.json\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m file:\n\u001B[0;32m     85\u001B[0m         json\u001B[38;5;241m.\u001B[39mdump(\u001B[38;5;28mlist\u001B[39m(not_found_financial_data_with_exception), file)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'prediction_subset' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Number of companies with not found financial data with exception: {len(not_found_financial_data_with_exception)}\")\n",
    "print(f\"Number of companies with not found financial data with webdriver timeout exception: {len(not_found_financial_data_with_webdriver_timeout_exception)}\")"
   ],
   "id": "9c459aed5de12f48"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Get companies with two tickers",
   "id": "df93be0a2931fe16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "companies_with_two_tickers = prediction_subset_with_gurufocus_data[prediction_subset_with_gurufocus_data['second_match_ticker'].notnull()]\n",
    "print(f\"Number of companies with two tickers: {len(companies_with_two_tickers)}\")\n",
    "print(companies_with_two_tickers.head(10))"
   ],
   "id": "53ad7a0b47cd8d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "companies_first_tickers = companies_with_two_tickers[['cik', 'company', 'ticker', 'gurufocus_stockid']]\n",
    "companies_first_tickers['second_match'] = False\n",
    "\n",
    "companies_second_tickers = companies_with_two_tickers[['cik', 'company', 'second_match_ticker', 'second_match_gurufocus_stockid']]\n",
    "companies_second_tickers.columns = ['cik', 'company', 'ticker', 'gurufocus_stockid']\n",
    "companies_second_tickers['second_match'] = True\n",
    "\n",
    "companies_with_two_tickers_flatten = pd.concat([companies_first_tickers, companies_second_tickers])\n",
    "\n",
    "print(len(companies_with_two_tickers_flatten))\n",
    "print(companies_with_two_tickers_flatten.head(10))"
   ],
   "id": "f5557f5a39cb2e20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "companies_with_two_tickers_flatten = companies_with_two_tickers_flatten.sort_values(by='ticker')\n",
    "print(companies_with_two_tickers_flatten.tail(10))"
   ],
   "id": "c8d05e2ff5ac9e0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "not_found_financial_data_with_webdriver_timeout_exception_df = pd.read_json('not_found_financial_data_with_webdriver_timeout_exception_two_ticker_case_2024-04-16_22-22-08.json')\n",
    "not_found_financial_data_with_webdriver_timeout_exception_df.columns = ['cik', 'gurufocus_stockid', 'ticker', 'exception']\n",
    "not_found_financial_data_with_webdriver_timeout_exception_df.drop(columns=['exception'], inplace=True)\n",
    "print(not_found_financial_data_with_webdriver_timeout_exception_df.head())"
   ],
   "id": "beef2b524c8cb188"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "companies_with_two_tickers_flatten = pd.read_csv('companies_with_two_tickers_flatten_2024-04-16_22-22-08.csv', index_col=0)\n",
    "print(companies_with_two_tickers_flatten.head(10))"
   ],
   "id": "9e04768702b3cae2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# login\n",
    "driver = webdriver.Edge(options=options)\n",
    "driver.get(login_url)\n",
    "\n",
    "username = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, 'username')))\n",
    "password = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.NAME, 'password')))\n",
    "username.clear()\n",
    "password.clear()\n",
    "username.send_keys(payload['username'])\n",
    "password.send_keys(payload['password'])\n",
    "password.send_keys(Keys.RETURN)\n",
    "\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.LINK_TEXT, 'Articles')))\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "not_found_financial_data_with_exception = []\n",
    "not_found_financial_data_with_webdriver_timeout_exception = []\n",
    "success_count = 0\n",
    "\n",
    "pbar = tqdm(not_found_financial_data_with_webdriver_timeout_exception_df.values.tolist())\n",
    "try:\n",
    "    for cik, stockid, ticker in pbar:\n",
    "        \n",
    "        url = f'https://www.gurufocus.com/stock/{stockid}/financials'\n",
    "        \n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(0.7)\n",
    "    \n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_per_share_data')))\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_ratios')))\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_income_statement')))\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_balance_sheet')))\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_cashflow_statement')))\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_valuation_ratios')))\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.ID, 'financial_table_valuation_and_quality')))\n",
    "            except Exception as ex:\n",
    "                not_found_financial_data_with_webdriver_timeout_exception.append([cik, stockid, ticker, str(ex)])\n",
    "                continue\n",
    "    \n",
    "            page_source = driver.page_source\n",
    "            with open(f'gurufocus-immediate-response-for-stockid-two-tickers-case/page_source-{cik}-{stockid}_{ticker}.txt', 'w', encoding='utf-8') as f:\n",
    "                f.write(page_source)\n",
    "            \n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "            gurufocus_company_name = soup.find('div', {'id': 'stock-header'}).find('div').text\n",
    "    \n",
    "            mask = (companies_with_two_tickers_flatten['cik'] == cik) & (companies_with_two_tickers_flatten['gurufocus_stockid'] == stockid)\n",
    "            companies_with_two_tickers_flatten.loc[mask, 'gurufocus-company-name'] = gurufocus_company_name\n",
    "\n",
    "            tables = soup.find_all('table')\n",
    "            \n",
    "            page_tables_ids = []\n",
    "            for table in tables:\n",
    "                try:\n",
    "                    table_id = table.get('id')\n",
    "                    if table_id:\n",
    "                        page_tables_ids.append(table_id)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if all(table_id not in page_tables_ids for table_id in table_ids):\n",
    "                pass\n",
    "            else:\n",
    "                merged_df = pd.DataFrame()\n",
    "                for table_id in table_ids:\n",
    "                    table = soup.find(id=table_id)\n",
    "                    df = pd.read_html(str(table), skiprows=1, header=0)[0]\n",
    "                    df = process_df(df)\n",
    "                    merged_df = pd.concat([merged_df, df])\n",
    "                    \n",
    "                merged_df.reset_index(drop=True, inplace=True)\n",
    "                merged_df.to_csv(f'financial_data_two_tickers_case/{cik}-{stockid}_{ticker}.csv')\n",
    "                \n",
    "                success_count += 1\n",
    "                pbar.set_description(f\"Success: {success_count}\")\n",
    "    \n",
    "        except Exception as ex:\n",
    "            not_found_financial_data_with_exception.append([cik, stockid, ticker, str(ex)])\n",
    "\n",
    "finally:\n",
    "    driver.close()\n",
    "    \n",
    "    datetime_now = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    \n",
    "    companies_with_two_tickers_flatten.to_csv(f'companies_with_two_tickers_flatten_{datetime_now}.csv')\n",
    "\n",
    "    with open(f'not_found_financial_data_with_exception_two_ticker_case_{datetime_now}.json', 'w') as file:\n",
    "        json.dump(list(not_found_financial_data_with_exception), file)\n",
    "        \n",
    "    with open(f'not_found_financial_data_with_webdriver_timeout_exception_two_ticker_case_{datetime_now}.json', 'w') as file:\n",
    "        json.dump(list(not_found_financial_data_with_webdriver_timeout_exception), file)"
   ],
   "id": "3531548943c3d864"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"Number of companies with not found financial data with exception: {len(not_found_financial_data_with_exception)}\")\n",
    "print(f\"Number of companies with not found financial data with webdriver timeout exception: {len(not_found_financial_data_with_webdriver_timeout_exception)}\")"
   ],
   "id": "a91b5a62a474212c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
